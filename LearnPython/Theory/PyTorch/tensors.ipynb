{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e01fb34",
   "metadata": {},
   "source": [
    "### 1. Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3bedc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty(1): tensor([3.8706e+25])\n",
      "empty(3): tensor([0., 0., 0.])\n",
      "empty(2,3): tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "empty(2, 2, 3): tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "rand(5,3): tensor([[0.8392, 0.1522, 0.9456],\n",
      "        [0.1599, 0.0984, 0.0883],\n",
      "        [0.2543, 0.7842, 0.8470],\n",
      "        [0.0730, 0.0693, 0.4106],\n",
      "        [0.3078, 0.0744, 0.3951]])\n",
      "zeros(5,3): tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# torch.empty(size): uninitiallized\n",
    "x = torch.empty(1) # scalar\n",
    "print(\"empty(1):\", x)\n",
    "x = torch.empty(3) # vector\n",
    "print(\"empty(3):\",x)\n",
    "x = torch.empty(2, 3) # matrix\n",
    "print(\"empty(2,3):\",x)\n",
    "x = torch.empty(2, 2, 3) # tensor, 3 dimensions\n",
    "#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
    "print(\"empty(2, 2, 3):\",x)\n",
    "\n",
    "# torch.rand(size): random numbers [0, 1]\n",
    "x = torch.rand(5, 3)\n",
    "print(\"rand(5,3):\", x)\n",
    "\n",
    "# torch.zeros(size), fill with 0\n",
    "# torch.ones(size), fill with 1\n",
    "x = torch.zeros(5, 3)\n",
    "print(\"zeros(5,3):\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a79378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size :  torch.Size([5, 3])\n",
      "Shape :  torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Check Size \n",
    "print(\"Size : \" , x.size())\n",
    "print(\"Shape : \" , x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28223ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float16)\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Check Datatype\n",
    "print(x.dtype)\n",
    "\n",
    "# Spcify types, float32 default\n",
    "x = torch.zeros(5,3, dtype=torch.float16)\n",
    "print(x)\n",
    "\n",
    "# Check Type\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90146c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 3]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Construct from data \n",
    "x  = torch.tensor([5, 5, 3 ])\n",
    "print(x, x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95eca6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Requires_grad argument \n",
    "# This will tell pytorch that it will need to calcuate the gradient for this tensor \n",
    "# Later in your optimization steps \n",
    "# i.e. this is a variable in your model that you want to optimize \n",
    "\n",
    "x = torch.tensor([5.5,3], requires_grad=True)\n",
    "print(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4030623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.8190, 0.5136],\n",
      "        [0.9709, 0.9184]])\n",
      "Addition :  tensor([[1.8190, 1.5136],\n",
      "        [1.9709, 1.9184]])\n",
      "Subtraction :  tensor([[0.1810, 0.4864],\n",
      "        [0.0291, 0.0816]])\n",
      "Multiplication :  tensor([[0.8190, 0.5136],\n",
      "        [0.9709, 0.9184]])\n",
      "Division :  tensor([[1.2210, 1.9472],\n",
      "        [1.0299, 1.0889]])\n"
     ]
    }
   ],
   "source": [
    "# Operations\n",
    "x = torch.ones(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "# Elementwise addition\n",
    "z = x + y\n",
    "torch.add(x,y)\n",
    "# torch.add(x,y)\n",
    "\n",
    "#In plane addition, everything with a  trailing underscore is a inplace operation\n",
    "#i.e. it will modify the variable\n",
    "# y.add_(x)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(\"Addition : \", z)\n",
    "\n",
    "z = x - y \n",
    "torch.sub(x, y)\n",
    "print(\"Subtraction : \", z)\n",
    "\n",
    "z = x * y \n",
    "torch.mul(x, y)\n",
    "print(\"Multiplication : \", z)\n",
    "\n",
    "z = x / y \n",
    "torch.div(x, y)\n",
    "print(\"Division : \", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5d2a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4594, 0.4578, 0.0236],\n",
      "        [0.7371, 0.0307, 0.0077],\n",
      "        [0.7398, 0.1254, 0.2807],\n",
      "        [0.7672, 0.6725, 0.4940],\n",
      "        [0.8361, 0.6619, 0.7829]])\n",
      "X[: , 0]  tensor([0.4594, 0.7371, 0.7398, 0.7672, 0.8361])\n",
      "X[1, :]  tensor([0.7371, 0.0307, 0.0077])\n",
      "X[1, 1]  tensor(0.0307)\n",
      "x[1, 1].item()  0.03069692850112915\n"
     ]
    }
   ],
   "source": [
    "# Slicing \n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(\"X[: , 0] \", x[: , 0 ])\n",
    "print(\"X[1, :] \", x[1, :  ])\n",
    "print(\"X[1, 1] \", x[1, 1])\n",
    "\n",
    "print(\"x[1, 1].item() \", x[1,1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "180a5db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# Reshape with torch.view()\n",
    "x = torch.randn(4, 4) \n",
    "y = x.view(16) # Shapes the x as (16, ) in y\n",
    "z = x.view(-1, 8) # The -1 tells PyTorch to infer the appropriate dimension automatically based on the total number of elements\n",
    "\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a34a89",
   "metadata": {},
   "source": [
    "### NumPy\n",
    "\n",
    "#### Converting a Torch to a NumPy array and vice versa is very easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0dd115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ed2a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Numpy to torch with .from_numpy(x), or torch.tensor() to copy it \n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "c = torch.tensor(a)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "\n",
    "# Again be careful when modifying\n",
    "a += 1 \n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7bd7b",
   "metadata": {},
   "source": [
    "### GPU Support\n",
    "\n",
    "#### By default are created on the CPU. But we can also move them to the GPU (if it's available), or create them direcly on the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8b2fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x = torch.rand(2, 2).to(device) # Move tensors to GPU device \n",
    "# x = a.to(\"cpu\")\n",
    "# x = x.to('cuda')\n",
    "\n",
    "x = torch.rand(2, 2, device=device) # or directly create them on GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486ef0a",
   "metadata": {},
   "source": [
    "### 2. Autograd\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. Generally speaking, torch.autograd is an engine for computing the vector-Jacobian product. It computes partial derivatives while applying the chain rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8c23f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1052,  0.8526,  0.8189], requires_grad=True)\n",
      "tensor([1.8948, 2.8526, 2.8189], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000021C98110F70>\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = True --> tracks all operations on the tensor. \n",
    "x = torch.randn(3, requires_grad=True )\n",
    "y = x + 2 \n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x)\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a886d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.7711, 24.4112, 23.8383], grad_fn=<MulBackward0>)\n",
      "tensor(19.6735, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations of y \n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc8c59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.7711, 24.4112, 23.8383], grad_fn=<MulBackward0>)\n",
      "tensor(19.6735, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the graidients with backpropogation\n",
    "# When we finish our computation we can  call .backward() and have all the gradients computer automatically. \n",
    "# The gradient for this tensor will be accumulated into .grad attribute. \n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "\n",
    "# ! ! ! Careful ! ! ! backward() accumulate the gradient for this tensor into .grad attribute. \n",
    "# ! ! ! We need to be careful during optimization ! ! ! optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b773b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7896, 5.7051, 5.6378])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cb610",
   "metadata": {},
   "source": [
    "## Stop a tensor from tracking history:\n",
    "\n",
    "For example during the training loop when we want to update our weights, or after training during evaluation. These operation part of the gradient computation. To prevent this, we can use:\n",
    "\n",
    "- x.requires_grad(False)\n",
    "- x.detach()\n",
    "- wrap in with torch.no_grad():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6615c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x0000021C978B0AC0>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad(...) changes on existing falg in-place \n",
    "a = torch.randn(2, 2)\n",
    "b = (a*a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "b = (a*a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05a01ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .deatch(): get a new tensor with the same content but no gradient computation: \n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "b = a.detach()\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b126a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# wrap in 'with torch.no_grad()'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad(): \n",
    "    b = a** 2\n",
    "    print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c0f6c",
   "metadata": {},
   "source": [
    "### Gradient Descent Autograd\n",
    "\n",
    "Linear Regression example:\n",
    "\n",
    "- _f(x) = w \\* x + b_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75a852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5.0) = 0.000\n"
     ]
    }
   ],
   "source": [
    "# Linear regression \n",
    "# f = w * x + b\n",
    "# here : f = 2 * x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32)  \n",
    "Y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16], dtype=torch.float32)  \n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output \n",
    "def forward(x): \n",
    "    return w * x \n",
    "\n",
    "# loss = MSE \n",
    "def loss(y, y_pred): \n",
    "    return ((y_pred - y)** 2).mean()\n",
    "\n",
    "x_test = 5.0\n",
    "print(f\"Prediction before traning: f({x_test}) = {forward(x_test).item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db78f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: f(5.0) = 51.000\n",
      "Prediction after training: f(5.0) = -158.100\n",
      "Prediction after training: f(5.0) = 699.210\n",
      "Prediction after training: f(5.0) = -2815.761\n",
      "Prediction after training: f(5.0) = 11595.620\n",
      "Prediction after training: f(5.0) = -47491.047\n",
      "Prediction after training: f(5.0) = 194764.297\n",
      "Prediction after training: f(5.0) = -798482.562\n",
      "Prediction after training: f(5.0) = 3273829.750\n",
      "epoch 10: w = -2684530.000, loss = 10932253097984.000\n",
      "Prediction after training: f(5.0) = -13422650.000\n",
      "Prediction after training: f(5.0) = 55032920.000\n",
      "Prediction after training: f(5.0) = -225634912.000\n",
      "Prediction after training: f(5.0) = 925103232.000\n",
      "Prediction after training: f(5.0) = -3792922880.000\n",
      "Prediction after training: f(5.0) = 15550984192.000\n",
      "Prediction after training: f(5.0) = -63759032320.000\n",
      "Prediction after training: f(5.0) = 261412028416.000\n",
      "Prediction after training: f(5.0) = -1071789375488.000\n",
      "Prediction after training: f(5.0) = 4394336911360.000\n",
      "epoch 20: w = -3603356188672.000, loss = 19696402610081940694368256.000\n",
      "Prediction after training: f(5.0) = -18016781729792.000\n",
      "Prediction after training: f(5.0) = 73868806979584.000\n",
      "Prediction after training: f(5.0) = -302862101905408.000\n",
      "Prediction after training: f(5.0) = 1241734772162560.000\n",
      "Prediction after training: f(5.0) = -5091112498757632.000\n",
      "Prediction after training: f(5.0) = 20873562533396480.000\n",
      "Prediction after training: f(5.0) = -85581612829376512.000\n",
      "Prediction after training: f(5.0) = 350884671870992384.000\n",
      "Prediction after training: f(5.0) = -1438627264622231552.000\n",
      "Prediction after training: f(5.0) = 5898371908646207488.000\n",
      "epoch 30: w = -4836665338923843584.000, loss = 35486610902789053865498709531295744000.000\n",
      "Prediction after training: f(5.0) = -24183327244375031808.000\n",
      "Prediction after training: f(5.0) = 99151636424281817088.000\n",
      "Prediction after training: f(5.0) = -406521677673620570112.000\n",
      "Prediction after training: f(5.0) = 1666738874943407128576.000\n",
      "Prediction after training: f(5.0) = -6833629781332936622080.000\n",
      "Prediction after training: f(5.0) = 28017881821990063439872.000\n",
      "Prediction after training: f(5.0) = -114873315695339241472000.000\n",
      "Prediction after training: f(5.0) = 470980612365289399517184.000\n",
      "Prediction after training: f(5.0) = -1931020301730663828029440.000\n",
      "Prediction after training: f(5.0) = 7917183453268503808704512.000\n",
      "epoch 40: w = -6492090616147613860233216.000, loss = inf\n",
      "Prediction after training: f(5.0) = -32460454233659573908013056.000\n",
      "Prediction after training: f(5.0) = 133087862819172854865592320.000\n",
      "Prediction after training: f(5.0) = -545660223723550649666764800.000\n",
      "Prediction after training: f(5.0) = 2237207101733998400729251840.000\n",
      "Prediction after training: f(5.0) = -9172549928766132686210203648.000\n",
      "Prediction after training: f(5.0) = 37607453291231199152568270848.000\n",
      "Prediction after training: f(5.0) = -154190554716154730229813739520.000\n",
      "Prediction after training: f(5.0) = 632181274336234393942236332032.000\n",
      "Prediction after training: f(5.0) = -2591943304114317927373208551424.000\n",
      "Prediction after training: f(5.0) = 10626968121108467819179013046272.000\n",
      "epoch 50: w = -8714114415414820634456211062784.000, loss = inf\n",
      "Prediction after training: f(5.0) = -43570571472611193364966467960832.000\n",
      "Prediction after training: f(5.0) = 178639351258401466175840906641408.000\n",
      "Prediction after training: f(5.0) = -732421344028008634087761076289536.000\n",
      "Prediction after training: f(5.0) = 3002927479566334417625313540308992.000\n",
      "Prediction after training: f(5.0) = -12312004461235028076065184118996992.000\n",
      "Prediction after training: f(5.0) = 50479218538651622968943309867712512.000\n",
      "Prediction after training: f(5.0) = -206964797493999701315123900336570368.000\n",
      "Prediction after training: f(5.0) = 848555725185112535377044306860703744.000\n",
      "Prediction after training: f(5.0) = -3479078481181777646472315417483280384.000\n",
      "Prediction after training: f(5.0) = 14264223167260948601588834858054975488.000\n",
      "epoch 60: w = -11696662794329881816786140344132567040.000, loss = inf\n",
      "Prediction after training: f(5.0) = -58483313971649409083930701720662835200.000\n",
      "Prediction after training: f(5.0) = inf\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 70: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 80: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 90: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 100: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "learning_rate = 0.1 \n",
    "n_epocha = 100\n",
    "\n",
    "for epoch in range(n_epocha): \n",
    "    # predict = forward pass \n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss \n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calcuate the gradients = backward pass \n",
    "    l.backward()\n",
    "\n",
    "    # Update weights \n",
    "    # w.data = w.data - learning_rate * w.grad \n",
    "    with torch.no_grad(): \n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero the gradients after updating \n",
    "    w.grad.zero_()\n",
    "\n",
    "    if(epoch+1) % 10 ==0 : \n",
    "        print(f'epoch {epoch + 1}: w = {w.item():.3f}, loss = {l.item():.3f}')\n",
    "\n",
    "    print(f'Prediction after training: f({x_test}) = {forward(x_test).item():.3f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
