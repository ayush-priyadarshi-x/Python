{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415bc26d",
   "metadata": {},
   "source": [
    "### Gradient Descent Autograd\n",
    "\n",
    "Linear Regression example:\n",
    "\n",
    "- _f(x) = w \\* x + b_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84bec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430339a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before traning: f(5.0) = 0.000\n"
     ]
    }
   ],
   "source": [
    "# Linear regression \n",
    "# f = w * x + b\n",
    "# here : f = 2 * x\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32)  \n",
    "Y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16], dtype=torch.float32)  \n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output \n",
    "def forward(x): \n",
    "    return w * x \n",
    "\n",
    "# loss = MSE \n",
    "def loss(y, y_pred): \n",
    "    return ((y_pred - y)** 2).mean()\n",
    "\n",
    "x_test = 5.0\n",
    "print(f\"Prediction before traning: f({x_test}) = {forward(x_test).item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709be415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training: f(5.0) = 51.000\n",
      "Prediction after training: f(5.0) = -158.100\n",
      "Prediction after training: f(5.0) = 699.210\n",
      "Prediction after training: f(5.0) = -2815.761\n",
      "Prediction after training: f(5.0) = 11595.620\n",
      "Prediction after training: f(5.0) = -47491.047\n",
      "Prediction after training: f(5.0) = 194764.297\n",
      "Prediction after training: f(5.0) = -798482.562\n",
      "Prediction after training: f(5.0) = 3273829.750\n",
      "epoch 10: w = -2684530.000, loss = 10932253097984.000\n",
      "Prediction after training: f(5.0) = -13422650.000\n",
      "Prediction after training: f(5.0) = 55032920.000\n",
      "Prediction after training: f(5.0) = -225634912.000\n",
      "Prediction after training: f(5.0) = 925103232.000\n",
      "Prediction after training: f(5.0) = -3792922880.000\n",
      "Prediction after training: f(5.0) = 15550984192.000\n",
      "Prediction after training: f(5.0) = -63759032320.000\n",
      "Prediction after training: f(5.0) = 261412028416.000\n",
      "Prediction after training: f(5.0) = -1071789375488.000\n",
      "Prediction after training: f(5.0) = 4394336911360.000\n",
      "epoch 20: w = -3603356188672.000, loss = 19696402610081940694368256.000\n",
      "Prediction after training: f(5.0) = -18016781729792.000\n",
      "Prediction after training: f(5.0) = 73868806979584.000\n",
      "Prediction after training: f(5.0) = -302862101905408.000\n",
      "Prediction after training: f(5.0) = 1241734772162560.000\n",
      "Prediction after training: f(5.0) = -5091112498757632.000\n",
      "Prediction after training: f(5.0) = 20873562533396480.000\n",
      "Prediction after training: f(5.0) = -85581612829376512.000\n",
      "Prediction after training: f(5.0) = 350884671870992384.000\n",
      "Prediction after training: f(5.0) = -1438627264622231552.000\n",
      "Prediction after training: f(5.0) = 5898371908646207488.000\n",
      "epoch 30: w = -4836665338923843584.000, loss = 35486610902789053865498709531295744000.000\n",
      "Prediction after training: f(5.0) = -24183327244375031808.000\n",
      "Prediction after training: f(5.0) = 99151636424281817088.000\n",
      "Prediction after training: f(5.0) = -406521677673620570112.000\n",
      "Prediction after training: f(5.0) = 1666738874943407128576.000\n",
      "Prediction after training: f(5.0) = -6833629781332936622080.000\n",
      "Prediction after training: f(5.0) = 28017881821990063439872.000\n",
      "Prediction after training: f(5.0) = -114873315695339241472000.000\n",
      "Prediction after training: f(5.0) = 470980612365289399517184.000\n",
      "Prediction after training: f(5.0) = -1931020301730663828029440.000\n",
      "Prediction after training: f(5.0) = 7917183453268503808704512.000\n",
      "epoch 40: w = -6492090616147613860233216.000, loss = inf\n",
      "Prediction after training: f(5.0) = -32460454233659573908013056.000\n",
      "Prediction after training: f(5.0) = 133087862819172854865592320.000\n",
      "Prediction after training: f(5.0) = -545660223723550649666764800.000\n",
      "Prediction after training: f(5.0) = 2237207101733998400729251840.000\n",
      "Prediction after training: f(5.0) = -9172549928766132686210203648.000\n",
      "Prediction after training: f(5.0) = 37607453291231199152568270848.000\n",
      "Prediction after training: f(5.0) = -154190554716154730229813739520.000\n",
      "Prediction after training: f(5.0) = 632181274336234393942236332032.000\n",
      "Prediction after training: f(5.0) = -2591943304114317927373208551424.000\n",
      "Prediction after training: f(5.0) = 10626968121108467819179013046272.000\n",
      "epoch 50: w = -8714114415414820634456211062784.000, loss = inf\n",
      "Prediction after training: f(5.0) = -43570571472611193364966467960832.000\n",
      "Prediction after training: f(5.0) = 178639351258401466175840906641408.000\n",
      "Prediction after training: f(5.0) = -732421344028008634087761076289536.000\n",
      "Prediction after training: f(5.0) = 3002927479566334417625313540308992.000\n",
      "Prediction after training: f(5.0) = -12312004461235028076065184118996992.000\n",
      "Prediction after training: f(5.0) = 50479218538651622968943309867712512.000\n",
      "Prediction after training: f(5.0) = -206964797493999701315123900336570368.000\n",
      "Prediction after training: f(5.0) = 848555725185112535377044306860703744.000\n",
      "Prediction after training: f(5.0) = -3479078481181777646472315417483280384.000\n",
      "Prediction after training: f(5.0) = 14264223167260948601588834858054975488.000\n",
      "epoch 60: w = -11696662794329881816786140344132567040.000, loss = inf\n",
      "Prediction after training: f(5.0) = -58483313971649409083930701720662835200.000\n",
      "Prediction after training: f(5.0) = inf\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 70: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 80: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 90: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "Prediction after training: f(5.0) = nan\n",
      "epoch 100: w = nan, loss = nan\n",
      "Prediction after training: f(5.0) = nan\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "learning_rate = 0.1 \n",
    "n_epocha = 100\n",
    "\n",
    "for epoch in range(n_epocha): \n",
    "    # predict = forward pass \n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # Loss \n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calcuate the gradients = backward pass \n",
    "    l.backward()\n",
    "\n",
    "    # Update weights \n",
    "    # w.data = w.data - learning_rate * w.grad \n",
    "    with torch.no_grad(): \n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero the gradients after updating \n",
    "    w.grad.zero_()\n",
    "\n",
    "    if(epoch+1) % 10 ==0 : \n",
    "        print(f'epoch {epoch + 1}: w = {w.item():.3f}, loss = {l.item():.3f}')\n",
    "\n",
    "    print(f'Prediction after training: f({x_test}) = {forward(x_test).item():.3f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
